{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Audios -> wav Audios\n",
    "Audio files ori -> Audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# 设定原始和目标文件夹的完整路径\n",
    "src_base_dir = \"E:/AMR/DA/Projekt/data/Audio_files_ori\"  # 原始文件夹\n",
    "dst_base_dir = \"E:/AMR/DA/Projekt/data/Audio_files\"  # 目标文件夹\n",
    "\n",
    "# 确保目标文件夹存在\n",
    "os.makedirs(dst_base_dir, exist_ok=True)\n",
    "\n",
    "# 正则表达式匹配 XC 编号格式，例如 \"XC123456\"\n",
    "xc_pattern = re.compile(r\"(XC\\d+)\")\n",
    "\n",
    "# 遍历原始文件夹\n",
    "for folder in os.listdir(src_base_dir):\n",
    "    src_folder_path = os.path.join(src_base_dir, folder)\n",
    "    dst_folder_path = os.path.join(dst_base_dir, folder)\n",
    "\n",
    "    # 确保目标子文件夹存在\n",
    "    os.makedirs(dst_folder_path, exist_ok=True)\n",
    "\n",
    "    # 确保是文件夹\n",
    "    if os.path.isdir(src_folder_path):\n",
    "        for file in os.listdir(src_folder_path):\n",
    "            if file.endswith((\".mp3\", \".ogg\", \".wav\")):  # 处理音频文件\n",
    "                match = xc_pattern.search(file)  # 提取 XC 编号\n",
    "                if match:\n",
    "                    number = match.group(1)  # 只保留 \"XC123456\"\n",
    "                    new_filename = f\"{number}.wav\"  # 统一改为 \"XC123456.wav\"\n",
    "\n",
    "                    # 原始文件路径和目标文件路径\n",
    "                    src_file_path = os.path.join(src_folder_path, file)\n",
    "                    dst_file_path = os.path.join(dst_folder_path, new_filename)\n",
    "\n",
    "                    # 复制并重命名\n",
    "                    shutil.copy2(src_file_path, dst_file_path)\n",
    "                    print(f\"已处理: {src_file_path} -> {dst_file_path}\")\n",
    "\n",
    "print(\"所有文件已重命名并移动至目标文件夹！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train meta csv Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "跳过文件夹：audio_file_counts.csv，命名格式不符合预期\n",
      "CSV 文件已保存至 E:/AMR/DA/Projekt/data/Audio_files/train_meta_100.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 设置数据文件夹路径\n",
    "base_dir = \"E:/AMR/DA/Projekt/data/Audio_files\"  # 这里可以改成你的实际路径\n",
    "\n",
    "# 用于存储数据\n",
    "data = []\n",
    "\n",
    "# 遍历 Audio_files 文件夹\n",
    "for folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    \n",
    "    # 只处理文件夹\n",
    "    if os.path.isdir(folder_path):\n",
    "        # 解析文件夹名称，获取 vocalization 和 bird_name\n",
    "        parts = folder.split(\" - \")\n",
    "        if len(parts) != 2:\n",
    "            print(f\"跳过文件夹：{folder}，命名格式不符合预期\")\n",
    "            continue\n",
    "        \n",
    "        vocalization, bird_name = parts\n",
    "        \n",
    "        # 遍历该类别下的所有音频文件\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".wav\"):  # 只处理 .wav 文件\n",
    "                number = file.replace(\".wav\", \"\")  # 提取 XC 编号\n",
    "                full_path = os.path.join(folder_path, file)  # 记录完整路径（绝对路径）\n",
    "                full_path = full_path.replace(\"\\\\\", \"/\")  # 统一路径分隔符\n",
    "                data.append([bird_name, vocalization, number, full_path])\n",
    "\n",
    "# 创建 DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"bird_name\", \"vocalization\", \"number\", \"path\"])\n",
    "\n",
    "# 保存 CSV\n",
    "csv_path = \"E:/AMR/DA/Projekt/data/Audio_files/train_meta_100.csv\"\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"CSV 文件已保存至 {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Split, Spectogram and all data meta.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import cv2\n",
    "import math\n",
    "import cupy as cp\n",
    "from cupyx.scipy import signal as cupy_signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 配置参数\n",
    "class config:\n",
    "    SEED = 2024\n",
    "    DEVICE = 'cpu'\n",
    "    OUTPUT_DIR = \"E:/AMR/DA/Projekt/data/Audio_spec\"  # 存储频谱数据\n",
    "    FS = 32000  # 采样率\n",
    "    N_FFT = 1095  # FFT 点数\n",
    "    WIN_SIZE = 412  # 频谱窗口大小\n",
    "    WIN_LAP = 100  # 频谱窗口重叠大小\n",
    "    MIN_FREQ = 40  # 最小频率\n",
    "    MAX_FREQ = 15000  # 最大频率\n",
    "    SEGMENT_DURATION = 3  # 每段 3 秒\n",
    "    SPEC_SIZE = (256, 256)  # 频谱图大小 (宽, 高)\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 读取 train_meta.csv\n",
    "csv_path = \"E:/AMR/DA/Projekt/data/Audio_files/train_meta_100_deduplicated.csv\"\n",
    "train_df = pd.read_csv(csv_path)\n",
    "\n",
    "# 频谱转换函数\n",
    "def oog2spec_via_cupy(audio_data):\n",
    "    audio_data = cp.array(audio_data)\n",
    "    \n",
    "    # 处理 NaN 数据\n",
    "    mean_signal = cp.nanmean(audio_data)\n",
    "    audio_data = cp.nan_to_num(audio_data, nan=mean_signal) if cp.isnan(audio_data).mean() < 1 else cp.zeros_like(audio_data)\n",
    "    \n",
    "    # 计算频谱\n",
    "    frequencies, times, spec_data = cupy_signal.spectrogram(\n",
    "        audio_data, \n",
    "        fs=config.FS, \n",
    "        nfft=config.N_FFT, \n",
    "        nperseg=config.WIN_SIZE, \n",
    "        noverlap=config.WIN_LAP, \n",
    "        window='hann'\n",
    "    )\n",
    "\n",
    "    # 过滤频率范围\n",
    "    valid_freq = (frequencies >= config.MIN_FREQ) & (frequencies <= config.MAX_FREQ)\n",
    "    spec_data = spec_data[valid_freq, :]\n",
    "    \n",
    "    # 对数变换和归一化\n",
    "    spec_data = cp.log10(spec_data + 1e-20)\n",
    "    spec_data = spec_data - spec_data.min()\n",
    "    spec_data = spec_data / spec_data.max()\n",
    "    \n",
    "    return spec_data.get()\n",
    "\n",
    "# 用于存储处理后的数据\n",
    "all_data = []\n",
    "\n",
    "# 处理音频数据\n",
    "for i, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "    file_path = row[\"path\"]\n",
    "    bird_name = row[\"bird_name\"]\n",
    "    vocalization = row[\"vocalization\"]\n",
    "    number = row[\"number\"]\n",
    "\n",
    "    # 读取音频\n",
    "    try:\n",
    "        audio_data, _ = librosa.load(file_path, sr=config.FS)\n",
    "    except Exception as e:\n",
    "        print(f\"加载失败: {file_path}, 错误: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 计算音频总时长\n",
    "    total_duration = len(audio_data) / config.FS\n",
    "\n",
    "    # 按 3s 分割音频\n",
    "    num_segments = math.floor(total_duration / config.SEGMENT_DURATION)\n",
    "\n",
    "    for seg_idx in range(num_segments):\n",
    "        start_idx = seg_idx * config.SEGMENT_DURATION * config.FS\n",
    "        end_idx = start_idx + config.SEGMENT_DURATION * config.FS\n",
    "        segment_audio = audio_data[start_idx:end_idx]\n",
    "\n",
    "        # 转换为频谱图\n",
    "        spec_data = oog2spec_via_cupy(segment_audio)\n",
    "\n",
    "        # 调整尺寸\n",
    "        spec_data = cv2.resize(spec_data, config.SPEC_SIZE, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # 保存频谱数据\n",
    "        spec_filename = f\"{number}_seg{seg_idx}.npy\"\n",
    "        spec_filepath = os.path.join(config.OUTPUT_DIR, spec_filename)\n",
    "        np.save(spec_filepath, spec_data.astype(np.float32))\n",
    "\n",
    "        # 记录数据\n",
    "        all_data.append([bird_name, vocalization, number, seg_idx, spec_filepath])\n",
    "\n",
    "# 生成 all_data_meta.csv\n",
    "meta_df = pd.DataFrame(all_data, columns=[\"bird_name\", \"vocalization\", \"number\", \"segment_index\", \"path\"])\n",
    "meta_csv_path = \"E:/AMR/DA/Projekt/data/all_data_meta.csv\"\n",
    "meta_df.to_csv(meta_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"所有音频处理完成，频谱图存储在 {config.OUTPUT_DIR}\")\n",
    "print(f\"元数据 CSV 文件已保存至 {meta_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新版生成spec和mel，512 256的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\40920\\.conda\\envs\\testenv\\lib\\site-packages\\cupyx\\jit\\_interface.py:173: FutureWarning: cupyx.jit.rawkernel is experimental. The interface can change in the future.\n",
      "  cupy._util.experimental('cupyx.jit.rawkernel')\n",
      "  5%|▌         | 421/7844 [01:52<24:08,  5.12it/s]  C:\\Users\\40920\\AppData\\Local\\Temp\\ipykernel_153280\\3010258197.py:96: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_data, _ = librosa.load(file_path, sr=config.FS)\n",
      "c:\\Users\\40920\\.conda\\envs\\testenv\\lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      " 22%|██▏       | 1692/7844 [09:27<19:42,  5.20it/s]  C:\\Users\\40920\\AppData\\Local\\Temp\\ipykernel_153280\\3010258197.py:82: RuntimeWarning: invalid value encountered in divide\n",
      "  mel_spec = mel_spec / mel_spec.max()\n",
      "100%|██████████| 7844/7844 [48:57<00:00,  2.67it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 所有音频处理完成！\n",
      "📁 线性频谱图存储于: E:/AMR/DA/Projekt/data/Audio_spec_paperstyle\n",
      "📁 Mel 频谱图存储于: E:/AMR/DA/Projekt/data/Audio_spec_mel\n",
      "📝 元数据 CSV 文件已保存至: E:/AMR/DA/Projekt/data/all_data_meta.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import cv2\n",
    "import math\n",
    "import cupy as cp\n",
    "from cupyx.scipy import signal as cupy_signal\n",
    "from tqdm import tqdm\n",
    "# 配置参数\n",
    "# class config:\n",
    "#     SEED = 2024\n",
    "#     DEVICE = 'cpu'\n",
    "#     OUTPUT_DIR = \"E:/AMR/DA/Projekt/data/Audio_spec\"  # 存储频谱数据\n",
    "#     FS = 32000  # 采样率\n",
    "#     N_FFT = 1095  # FFT 点数\n",
    "#     WIN_SIZE = 412  # 频谱窗口大小\n",
    "#     WIN_LAP = 100  # 频谱窗口重叠大小\n",
    "#     MIN_FREQ = 40  # 最小频率\n",
    "#     MAX_FREQ = 15000  # 最大频率\n",
    "#     SEGMENT_DURATION = 3  # 每段 3 秒\n",
    "#     SPEC_SIZE = (256, 256)  # 频谱图大小 (宽, 高)\n",
    "# 配置参数\n",
    "class config:\n",
    "    SEED = 2024\n",
    "    DEVICE = 'cpu'\n",
    "    OUTPUT_DIR_SPEC = \"E:/AMR/DA/Projekt/data/Audio_spec_paperstyle\"  # 线性频谱图目录\n",
    "    OUTPUT_DIR_MEL = \"E:/AMR/DA/Projekt/data/Audio_spec_mel\"         # mel频谱图目录\n",
    "    FS = 48000\n",
    "    N_FFT = 512\n",
    "    WIN_SIZE = 512\n",
    "    WIN_LAP = 384\n",
    "    MIN_FREQ = 150\n",
    "    MAX_FREQ = 15000\n",
    "    SEGMENT_DURATION = 3\n",
    "    SPEC_SIZE = (512, 256)  # (宽, 高)\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(config.OUTPUT_DIR_SPEC, exist_ok=True)\n",
    "os.makedirs(config.OUTPUT_DIR_MEL, exist_ok=True)\n",
    "\n",
    "# 读取 train_meta.csv\n",
    "csv_path = \"E:/AMR/DA/Projekt/data/Audio_files/train_meta_100_deduplicated.csv\"\n",
    "train_df = pd.read_csv(csv_path)\n",
    "\n",
    "# 线性频谱图（via cupy）\n",
    "def oog2spec_via_cupy(audio_data):\n",
    "    audio_data = cp.array(audio_data)\n",
    "    mean_signal = cp.nanmean(audio_data)\n",
    "    audio_data = cp.nan_to_num(audio_data, nan=mean_signal) if cp.isnan(audio_data).mean() < 1 else cp.zeros_like(audio_data)\n",
    "    \n",
    "    frequencies, times, spec_data = cupy_signal.spectrogram(\n",
    "        audio_data,\n",
    "        fs=config.FS,\n",
    "        nfft=config.N_FFT,\n",
    "        nperseg=config.WIN_SIZE,\n",
    "        noverlap=config.WIN_LAP,\n",
    "        window='hann'\n",
    "    )\n",
    "    valid_freq = (frequencies >= config.MIN_FREQ) & (frequencies <= config.MAX_FREQ)\n",
    "    spec_data = spec_data[valid_freq, :]\n",
    "    spec_data = cp.log10(spec_data + 1e-20)\n",
    "    spec_data = spec_data - spec_data.min()\n",
    "    spec_data = spec_data / spec_data.max()\n",
    "    return spec_data.get()\n",
    "\n",
    "# 新增：mel 频谱图\n",
    "def audio2mel(audio_data):\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=config.FS,\n",
    "        n_fft=config.N_FFT,\n",
    "        hop_length=config.N_FFT - config.WIN_LAP,\n",
    "        win_length=config.WIN_SIZE,\n",
    "        window='hann',\n",
    "        n_mels=64,\n",
    "        fmin=config.MIN_FREQ,\n",
    "        fmax=config.MAX_FREQ\n",
    "    )\n",
    "    mel_spec = np.log10(mel_spec + 1e-9)\n",
    "    mel_spec = mel_spec - mel_spec.min()\n",
    "    mel_spec = mel_spec / mel_spec.max()\n",
    "    return mel_spec\n",
    "\n",
    "# 存储处理结果\n",
    "all_data = []\n",
    "\n",
    "# 主处理循环\n",
    "for i, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "    file_path = row[\"path\"]\n",
    "    bird_name = row[\"bird_name\"]\n",
    "    vocalization = row[\"vocalization\"]\n",
    "    number = row[\"number\"]\n",
    "\n",
    "    try:\n",
    "        audio_data, _ = librosa.load(file_path, sr=config.FS)\n",
    "    except Exception as e:\n",
    "        print(f\"加载失败: {file_path}, 错误: {e}\")\n",
    "        continue\n",
    "\n",
    "    total_duration = len(audio_data) / config.FS\n",
    "    num_segments = math.floor(total_duration / config.SEGMENT_DURATION)\n",
    "\n",
    "    for seg_idx in range(num_segments):\n",
    "        start_idx = seg_idx * config.SEGMENT_DURATION * config.FS\n",
    "        end_idx = start_idx + config.SEGMENT_DURATION * config.FS\n",
    "        segment_audio = audio_data[start_idx:end_idx]\n",
    "\n",
    "        # ➤ 生成线性频谱图\n",
    "        spec_data = oog2spec_via_cupy(segment_audio)\n",
    "        spec_data = cv2.resize(spec_data, config.SPEC_SIZE, interpolation=cv2.INTER_AREA)\n",
    "        spec_filename = f\"{number}_seg{seg_idx}.npy\"\n",
    "        spec_filepath = os.path.join(config.OUTPUT_DIR_SPEC, spec_filename)\n",
    "        np.save(spec_filepath, spec_data.astype(np.float32))\n",
    "\n",
    "        # ➤ 生成 mel 频谱图\n",
    "        mel_data = audio2mel(segment_audio)\n",
    "        mel_data = cv2.resize(mel_data, config.SPEC_SIZE, interpolation=cv2.INTER_AREA)\n",
    "        mel_filename = f\"{number}_seg{seg_idx}_mel.npy\"\n",
    "        mel_filepath = os.path.join(config.OUTPUT_DIR_MEL, mel_filename)\n",
    "        np.save(mel_filepath, mel_data.astype(np.float32))\n",
    "\n",
    "        # ➤ 记录路径信息\n",
    "        all_data.append([bird_name, vocalization, number, seg_idx, spec_filepath, mel_filepath])\n",
    "\n",
    "# 保存元数据\n",
    "meta_df = pd.DataFrame(all_data, columns=[\n",
    "    \"bird_name\", \"vocalization\", \"number\", \"segment_index\", \"spec_path\", \"mel_path\"\n",
    "])\n",
    "meta_csv_path = \"E:/AMR/DA/Projekt/data/all_data_meta.csv\"\n",
    "meta_df.to_csv(meta_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ 所有音频处理完成！\")\n",
    "print(f\"📁 线性频谱图存储于: {config.OUTPUT_DIR_SPEC}\")\n",
    "print(f\"📁 Mel 频谱图存储于: {config.OUTPUT_DIR_MEL}\")\n",
    "print(f\"📝 元数据 CSV 文件已保存至: {meta_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 整合freefield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 配置参数\n",
    "source_dir = \"E:/AMR/DA/Projekt/data/freefield1010\"  # 原始目录\n",
    "target_dir = \"E:/AMR/DA/Projekt/data/negative_audio\"  # 目标目录\n",
    "\n",
    "# 确保目标目录存在\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# 遍历原始目录及其子目录\n",
    "for root, _, files in os.walk(source_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):  # 只处理 .wav 文件\n",
    "            source_file = os.path.join(root, file)  # 源文件路径\n",
    "            target_file = os.path.join(target_dir, file)  # 目标文件路径\n",
    "\n",
    "            # 如果目标文件夹中没有这个文件，则复制\n",
    "            if not os.path.exists(target_file):\n",
    "                shutil.copy2(source_file, target_file)  # 使用 copy2 保留文件的原始元数据\n",
    "                print(f\"复制 {source_file} 到 {target_file}\")\n",
    "            else:\n",
    "                print(f\"跳过 {file}，目标文件已存在。\")\n",
    "\n",
    "print(\"音频文件传输完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Negative Samples from freefield1010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import cv2\n",
    "import math\n",
    "import cupy as cp\n",
    "from cupyx.scipy import signal as cupy_signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 配置参数\n",
    "class config:\n",
    "    SEED = 2024\n",
    "    DEVICE = 'cpu'\n",
    "    OUTPUT_DIR = \"E:/AMR/DA/Projekt/data/Audio_spec\"  # 存储频谱数据\n",
    "    FS = 32000  # 采样率\n",
    "    N_FFT = 1095  # FFT 点数\n",
    "    WIN_SIZE = 412  # 频谱窗口大小\n",
    "    WIN_LAP = 100  # 频谱窗口重叠大小\n",
    "    MIN_FREQ = 40  # 最小频率\n",
    "    MAX_FREQ = 15000  # 最大频率\n",
    "    SEGMENT_DURATION = 3  # 每段 3 秒\n",
    "    SPEC_SIZE = (256, 256)  # 频谱图大小 (宽, 高)\n",
    "    MAX_NEGATIVE_SAMPLES = 3500  # 最大负样本数量\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 频谱转换函数\n",
    "def oog2spec_via_cupy(audio_data):\n",
    "    if len(audio_data) == 0:\n",
    "        print(\"⚠️ 警告：音频数据为空，跳过该文件。\")\n",
    "        return None\n",
    "    \n",
    "    audio_data = cp.array(audio_data)\n",
    "    \n",
    "    # 计算频谱\n",
    "    frequencies, times, spec_data = cupy_signal.spectrogram(\n",
    "        audio_data, \n",
    "        fs=config.FS, \n",
    "        nfft=config.N_FFT, \n",
    "        nperseg=config.WIN_SIZE, \n",
    "        noverlap=config.WIN_LAP, \n",
    "        window='hann'\n",
    "    )\n",
    "\n",
    "    # 过滤频率范围\n",
    "    valid_freq = (frequencies >= config.MIN_FREQ) & (frequencies <= config.MAX_FREQ)\n",
    "    spec_data = spec_data[valid_freq, :]\n",
    "    \n",
    "    # 对数变换和归一化\n",
    "    spec_data = cp.log10(spec_data + 1e-20)\n",
    "    spec_data = spec_data - spec_data.min()\n",
    "    spec_data = spec_data / spec_data.max()\n",
    "    \n",
    "    return spec_data.get()\n",
    "\n",
    "# 处理背景噪音（negative samples）数据\n",
    "negative_samples_dir = \"E:/AMR/DA/Projekt/data/negative_audio\"  # 背景噪音音频目录\n",
    "negative_samples_processed = 0\n",
    "negative_data = []  # 存储负样本信息\n",
    "\n",
    "for root, _, files in os.walk(negative_samples_dir):\n",
    "    for file in tqdm(files, desc=\"Processing Negative Audio Files\"):\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            try:\n",
    "                # 加载音频\n",
    "                audio_data, _ = librosa.load(file_path, sr=config.FS)\n",
    "\n",
    "                if len(audio_data) == 0:\n",
    "                    print(f\"⚠️ 警告：文件 {file} 音频数据为空，跳过。\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 加载失败: {file}, 错误: {e}\")\n",
    "                continue\n",
    "\n",
    "            # 计算音频总时长\n",
    "            total_duration = len(audio_data) / config.FS\n",
    "\n",
    "            # 计算完整的 3 秒片段数量\n",
    "            num_segments = math.floor(total_duration / config.SEGMENT_DURATION)\n",
    "\n",
    "            for seg_idx in range(num_segments + 1):  # +1 以考虑最后一个不足3秒的片段\n",
    "                start_idx = seg_idx * config.SEGMENT_DURATION * config.FS\n",
    "                end_idx = start_idx + config.SEGMENT_DURATION * config.FS\n",
    "                segment_audio = audio_data[start_idx:end_idx]\n",
    "\n",
    "                # 如果最后一个片段不足 3s，则进行 Padding（零填充）\n",
    "                if len(segment_audio) < config.SEGMENT_DURATION * config.FS:\n",
    "                    padding = config.SEGMENT_DURATION * config.FS - len(segment_audio)\n",
    "                    segment_audio = np.pad(segment_audio, (0, padding), mode='constant', constant_values=0)\n",
    "\n",
    "                # 生成频谱图\n",
    "                spec_data = oog2spec_via_cupy(segment_audio)\n",
    "\n",
    "                if spec_data is None:\n",
    "                    print(f\"⚠️ 频谱数据为空，跳过文件 {file} 的片段 {seg_idx}。\")\n",
    "                    continue  # 跳过该片段\n",
    "\n",
    "                # 调整尺寸\n",
    "                spec_data = cv2.resize(spec_data, config.SPEC_SIZE, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                # 保存频谱数据\n",
    "                spec_filename = f\"negative_{file.split('.')[0]}_seg{seg_idx}.npy\"\n",
    "                spec_filepath = os.path.join(config.OUTPUT_DIR, spec_filename)\n",
    "                np.save(spec_filepath, spec_data.astype(np.float32))\n",
    "\n",
    "                # 记录数据\n",
    "                negative_data.append([\"Background Noise\", \"none\", f\"negative_{file.split('.')[0]}\", seg_idx, spec_filepath])\n",
    "                negative_samples_processed += 1\n",
    "\n",
    "                # 达到最大数量后停止\n",
    "                if negative_samples_processed >= config.MAX_NEGATIVE_SAMPLES:\n",
    "                    print(f\"🎯 已处理 {config.MAX_NEGATIVE_SAMPLES} 个负样本，停止处理。\")\n",
    "                    break\n",
    "\n",
    "            if negative_samples_processed >= config.MAX_NEGATIVE_SAMPLES:\n",
    "                break\n",
    "\n",
    "    if negative_samples_processed >= config.MAX_NEGATIVE_SAMPLES:\n",
    "        break\n",
    "\n",
    "# 读取原始 all_data_meta.csv\n",
    "meta_csv_path = \"E:/AMR/DA/Projekt/data/all_data_meta.csv\"\n",
    "if os.path.exists(meta_csv_path):\n",
    "    existing_df = pd.read_csv(meta_csv_path)\n",
    "    negative_df = pd.DataFrame(negative_data, columns=[\"bird_name\", \"vocalization\", \"number\", \"segment_index\", \"path\"])\n",
    "    combined_df = pd.concat([existing_df, negative_df], ignore_index=True)\n",
    "else:\n",
    "    print(\"⚠️ 未找到原始 all_data_meta.csv，生成新的文件。\")\n",
    "    combined_df = pd.DataFrame(negative_data, columns=[\"bird_name\", \"vocalization\", \"number\", \"segment_index\", \"path\"])\n",
    "\n",
    "# 生成新的 CSV 文件\n",
    "new_meta_csv_path = \"E:/AMR/DA/Projekt/data/all_data_meta_with_negative.csv\"\n",
    "combined_df.to_csv(new_meta_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n🎯 负样本处理完成！\")\n",
    "print(f\"✅ 生成了 {negative_samples_processed} 个负样本（最大 {config.MAX_NEGATIVE_SAMPLES}）\")\n",
    "print(f\"📄 频谱数据存储在 {config.OUTPUT_DIR}\")\n",
    "print(f\"📊 新的元数据 CSV 文件已保存：{new_meta_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新版生成负样本spec和mel，并添加到all data meta 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Negative Audio Files:  11%|█▏        | 874/7690 [01:04<08:20, 13.61it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 达到最大负样本数 3500，停止处理。\n",
      "\n",
      "✅ 负样本处理完成，生成样本数：3500\n",
      "📁 线性谱存储路径: E:/AMR/DA/Projekt/data/Audio_spec_paperstyle\n",
      "📁 mel谱存储路径: E:/AMR/DA/Projekt/data/Audio_spec_mel\n",
      "📝 新 CSV 已保存: E:/AMR/DA/Projekt/data/all_data_meta_with_negative.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import cv2\n",
    "import math\n",
    "import cupy as cp\n",
    "from cupyx.scipy import signal as cupy_signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 配置参数（和主数据一致）\n",
    "class config:\n",
    "    SEED = 2024\n",
    "    DEVICE = 'cpu'\n",
    "    OUTPUT_DIR_SPEC = \"E:/AMR/DA/Projekt/data/Audio_spec_paperstyle\"\n",
    "    OUTPUT_DIR_MEL = \"E:/AMR/DA/Projekt/data/Audio_spec_mel\"\n",
    "    FS = 48000\n",
    "    N_FFT = 512\n",
    "    WIN_SIZE = 512\n",
    "    WIN_LAP = 384\n",
    "    MIN_FREQ = 150\n",
    "    MAX_FREQ = 15000\n",
    "    SEGMENT_DURATION = 3\n",
    "    SPEC_SIZE = (512, 256)\n",
    "    MAX_NEGATIVE_SAMPLES = 3500\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(config.OUTPUT_DIR_SPEC, exist_ok=True)\n",
    "os.makedirs(config.OUTPUT_DIR_MEL, exist_ok=True)\n",
    "\n",
    "# ➤ 线性频谱图生成\n",
    "def oog2spec_via_cupy(audio_data):\n",
    "    if len(audio_data) == 0:\n",
    "        return None\n",
    "    audio_data = cp.array(audio_data)\n",
    "    frequencies, times, spec_data = cupy_signal.spectrogram(\n",
    "        audio_data,\n",
    "        fs=config.FS,\n",
    "        nfft=config.N_FFT,\n",
    "        nperseg=config.WIN_SIZE,\n",
    "        noverlap=config.WIN_LAP,\n",
    "        window='hann'\n",
    "    )\n",
    "    valid_freq = (frequencies >= config.MIN_FREQ) & (frequencies <= config.MAX_FREQ)\n",
    "    spec_data = spec_data[valid_freq, :]\n",
    "    spec_data = cp.log10(spec_data + 1e-20)\n",
    "    spec_data = (spec_data - spec_data.min()) / (spec_data.max() - spec_data.min() + 1e-9)\n",
    "    return spec_data.get()\n",
    "\n",
    "# ➤ mel 频谱图生成\n",
    "def audio2mel(audio_data):\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=config.FS,\n",
    "        n_fft=config.N_FFT,\n",
    "        hop_length=config.N_FFT - config.WIN_LAP,\n",
    "        win_length=config.WIN_SIZE,\n",
    "        window='hann',\n",
    "        n_mels=64,\n",
    "        fmin=config.MIN_FREQ,\n",
    "        fmax=config.MAX_FREQ\n",
    "    )\n",
    "    mel_spec = np.log10(mel_spec + 1e-9)\n",
    "    mel_spec = (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min() + 1e-9)\n",
    "    return mel_spec\n",
    "\n",
    "# ➤ 处理负样本\n",
    "negative_samples_dir = \"E:/AMR/DA/Projekt/data/negative_audio\"\n",
    "negative_samples_processed = 0\n",
    "negative_data = []\n",
    "\n",
    "for root, _, files in os.walk(negative_samples_dir):\n",
    "    for file in tqdm(files, desc=\"Processing Negative Audio Files\"):\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            try:\n",
    "                audio_data, _ = librosa.load(file_path, sr=config.FS)\n",
    "                if len(audio_data) == 0:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 加载失败: {file}, 错误: {e}\")\n",
    "                continue\n",
    "\n",
    "            total_duration = len(audio_data) / config.FS\n",
    "            num_segments = math.floor(total_duration / config.SEGMENT_DURATION)\n",
    "\n",
    "            for seg_idx in range(num_segments + 1):  # 包含最后片段\n",
    "                start_idx = seg_idx * config.SEGMENT_DURATION * config.FS\n",
    "                end_idx = start_idx + config.SEGMENT_DURATION * config.FS\n",
    "                segment_audio = audio_data[start_idx:end_idx]\n",
    "\n",
    "                # 填充不足 3 秒的片段\n",
    "                if len(segment_audio) < config.SEGMENT_DURATION * config.FS:\n",
    "                    padding = config.SEGMENT_DURATION * config.FS - len(segment_audio)\n",
    "                    segment_audio = np.pad(segment_audio, (0, padding), mode='constant')\n",
    "\n",
    "                # ➤ 生成线性谱图\n",
    "                spec_data = oog2spec_via_cupy(segment_audio)\n",
    "                if spec_data is None:\n",
    "                    continue\n",
    "                spec_data = cv2.resize(spec_data, config.SPEC_SIZE, interpolation=cv2.INTER_AREA)\n",
    "                spec_filename = f\"negative_{file.split('.')[0]}_seg{seg_idx}.npy\"\n",
    "                spec_path = os.path.join(config.OUTPUT_DIR_SPEC, spec_filename)\n",
    "                np.save(spec_path, spec_data.astype(np.float32))\n",
    "\n",
    "                # ➤ 生成 mel 频谱图\n",
    "                mel_data = audio2mel(segment_audio)\n",
    "                mel_data = cv2.resize(mel_data, config.SPEC_SIZE, interpolation=cv2.INTER_AREA)\n",
    "                mel_filename = f\"negative_{file.split('.')[0]}_seg{seg_idx}_mel.npy\"\n",
    "                mel_path = os.path.join(config.OUTPUT_DIR_MEL, mel_filename)\n",
    "                np.save(mel_path, mel_data.astype(np.float32))\n",
    "\n",
    "                # ➤ 记录路径信息\n",
    "                negative_data.append([\"Background Noise\", \"none\", f\"negative_{file.split('.')[0]}\", seg_idx, spec_path, mel_path])\n",
    "                negative_samples_processed += 1\n",
    "\n",
    "                if negative_samples_processed >= config.MAX_NEGATIVE_SAMPLES:\n",
    "                    print(f\"🎯 达到最大负样本数 {config.MAX_NEGATIVE_SAMPLES}，停止处理。\")\n",
    "                    break\n",
    "\n",
    "            if negative_samples_processed >= config.MAX_NEGATIVE_SAMPLES:\n",
    "                break\n",
    "    if negative_samples_processed >= config.MAX_NEGATIVE_SAMPLES:\n",
    "        break\n",
    "\n",
    "# ➤ 合并 CSV\n",
    "meta_csv_path = \"E:/AMR/DA/Projekt/data/all_data_meta.csv\"\n",
    "new_meta_csv_path = \"E:/AMR/DA/Projekt/data/all_data_meta_with_negative.csv\"\n",
    "\n",
    "if os.path.exists(meta_csv_path):\n",
    "    existing_df = pd.read_csv(meta_csv_path)\n",
    "    negative_df = pd.DataFrame(negative_data, columns=[\"bird_name\", \"vocalization\", \"number\", \"segment_index\", \"spec_path\", \"mel_path\"])\n",
    "    combined_df = pd.concat([existing_df, negative_df], ignore_index=True)\n",
    "else:\n",
    "    print(\"⚠️ 未找到原始 CSV，仅使用负样本生成新文件。\")\n",
    "    combined_df = pd.DataFrame(negative_data, columns=[\"bird_name\", \"vocalization\", \"number\", \"segment_index\", \"spec_path\", \"mel_path\"])\n",
    "\n",
    "# ➤ 保存新 CSV\n",
    "combined_df.to_csv(new_meta_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n✅ 负样本处理完成，生成样本数：{negative_samples_processed}\")\n",
    "print(f\"📁 线性谱存储路径: {config.OUTPUT_DIR_SPEC}\")\n",
    "print(f\"📁 mel谱存储路径: {config.OUTPUT_DIR_MEL}\")\n",
    "print(f\"📝 新 CSV 已保存: {new_meta_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectogram.npy Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 开始检查 .npy 文件...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\40920\\.conda\\envs\\testenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'path'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_csv_path)\n\u001b[0;32m     52\u001b[0m blacklist \u001b[38;5;241m=\u001b[39m load_blacklist()\n\u001b[1;32m---> 53\u001b[0m invalid_files \u001b[38;5;241m=\u001b[39m \u001b[43mfind_invalid_npy_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblacklist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m save_blacklist(invalid_files)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 过滤无效数据\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m, in \u001b[0;36mfind_invalid_npy_files\u001b[1;34m(df, blacklist)\u001b[0m\n\u001b[0;32m     27\u001b[0m invalid_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(blacklist)  \u001b[38;5;66;03m# 先加载已有的黑名单\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔍 开始检查 .npy 文件...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m扫描无效数据\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m invalid_files:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# 跳过已知无效文件\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\40920\\.conda\\envs\\testenv\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\40920\\.conda\\envs\\testenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'path'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 📌 文件路径\n",
    "data_csv_path = \"E:/AMR/DA/Projekt/data/all_data_meta_with_negative.csv\"  # 原始数据\n",
    "clean_data_csv_path = \"E:/AMR/DA/Projekt/data/all_data_meta_clean.csv\"  # 过滤后数据\n",
    "blacklist_path = \"E:/AMR/DA/Projekt/data/blacklist.txt\"  # 黑名单文件\n",
    "\n",
    "# 读取黑名单（如果存在）\n",
    "def load_blacklist():\n",
    "    if os.path.exists(blacklist_path):\n",
    "        with open(blacklist_path, \"r\") as f:\n",
    "            return set(line.strip() for line in f.readlines())\n",
    "    return set()\n",
    "\n",
    "# 保存黑名单\n",
    "def save_blacklist(blacklist):\n",
    "    with open(blacklist_path, \"w\") as f:\n",
    "        for file in blacklist:\n",
    "            f.write(file + \"\\n\")\n",
    "    print(f\"✅ 黑名单已更新，共 {len(blacklist)} 个无效文件\")\n",
    "\n",
    "# 检查 `.npy` 文件是否包含 NaN/Inf\n",
    "def find_invalid_npy_files(df, blacklist):\n",
    "    invalid_files = set(blacklist)  # 先加载已有的黑名单\n",
    "\n",
    "    print(\"🔍 开始检查 .npy 文件...\")\n",
    "    for file_path in tqdm(df[\"path\"], desc=\"扫描无效数据\"):\n",
    "        if file_path in invalid_files:\n",
    "            continue  # 跳过已知无效文件\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"❌ 文件不存在: {file_path}\")\n",
    "            invalid_files.add(file_path)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            spectrogram = np.load(file_path)\n",
    "            if np.isnan(spectrogram).any() or np.isinf(spectrogram).any():\n",
    "                print(f\"❌ 发现 NaN/Inf: {file_path}\")\n",
    "                invalid_files.add(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 读取失败: {file_path}, 错误: {e}\")\n",
    "            invalid_files.add(file_path)\n",
    "\n",
    "    return invalid_files\n",
    "\n",
    "# 运行数据过滤\n",
    "df = pd.read_csv(data_csv_path)\n",
    "blacklist = load_blacklist()\n",
    "invalid_files = find_invalid_npy_files(df, blacklist)\n",
    "save_blacklist(invalid_files)\n",
    "\n",
    "# 过滤无效数据\n",
    "df_clean = df[~df[\"path\"].isin(invalid_files)]\n",
    "df_clean.to_csv(clean_data_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ 已过滤无效数据，生成 {clean_data_csv_path}（保留 {len(df_clean)} 条数据）\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 修改标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm  # 进度条，方便观察进度\n",
    "\n",
    "# 定义 hasBird() 方法\n",
    "def hasBird(spec, threshold=16):\n",
    "    img = spec.copy()\n",
    "    \n",
    "    # STEP 1: Median blur\n",
    "    img = cv2.medianBlur(img, 5)\n",
    "    \n",
    "    # STEP 2: Median threshold\n",
    "    col_median = np.median(img, axis=0, keepdims=True)\n",
    "    row_median = np.median(img, axis=1, keepdims=True)\n",
    "    img[img < row_median * 1.2] = 0\n",
    "    img[img < col_median * 1.2] = 0\n",
    "    img[img > 0] = 1\n",
    "    \n",
    "    # STEP 3: Remove isolated pixels\n",
    "    struct = np.ones((3, 3))\n",
    "    id_regions, num_ids = ndimage.label(img, structure=struct)\n",
    "    id_sizes = np.array(ndimage.sum(img, id_regions, range(num_ids + 1)))\n",
    "    area_mask = (id_sizes == 1)\n",
    "    img[area_mask[id_regions]] = 0\n",
    "    \n",
    "    # STEP 4: Morphological closing\n",
    "    img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, np.ones((5, 5), np.float32))\n",
    "    \n",
    "    # STEP 5: Frequency crop (keeping middle frequency range)\n",
    "    img = img[8:-85, :]\n",
    "    \n",
    "    # STEP 6: Count active rows\n",
    "    row_max = np.max(img, axis=1)\n",
    "    row_max = ndimage.binary_dilation(row_max, iterations=2).astype(row_max.dtype)\n",
    "    rthresh = row_max.sum()\n",
    "    \n",
    "    # STEP 7: Apply threshold\n",
    "    return rthresh >= threshold\n",
    "\n",
    "# 读取 CSV 文件\n",
    "csv_path = \"E:/AMR/DA/Projekt/data/valid_list_for_zoom006_100_nofreefiled.csv\"\n",
    "output_csv_path = \"E:/AMR/DA/Projekt/data/vliad_list_for_zoom006_100_nofreefiled_refine_th12.csv\"\n",
    "counter = 0\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 遍历 CSV，处理每个频谱图\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    # 如果 bird_name 已经是 \"Background Noise\"，跳过检测\n",
    "    if row[\"bird_name\"] == \"Background Noise\":\n",
    "        continue  \n",
    "\n",
    "    spec_path = row[\"path\"].replace(\"\\\\\", \"/\")  # 兼容 Windows 路径\n",
    "    try:\n",
    "        # 读取 .npy 频谱数据\n",
    "        spec_data = np.load(spec_path)\n",
    "        \n",
    "        # 判断是否有鸟声\n",
    "        if not hasBird(spec_data):\n",
    "            df.at[idx, \"bird_name\"] = \"Background Noise\"\n",
    "            df.at[idx, \"vocalization\"] = \"none\"\n",
    "            counter += 1\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 无法处理文件: {spec_path}, 错误: {e}\")\n",
    "\n",
    "# 保存新的 CSV 文件\n",
    "df.to_csv(output_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ 处理完成，已保存至 {output_csv_path}\")\n",
    "print(f\"需要修改的数量：{counter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 可视化中间步骤\n",
    "def visualize_step(title, img):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(img, aspect='auto', cmap='magma', origin='lower')\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# 选择一个 `.npy` 样本路径（你可以手动改成你想测试的文件）\n",
    "spec_path = \"E:/AMR/DA/Projekt/data/Audio_spec\\XC864575_seg28.npy\"\n",
    "\n",
    "spec_data = np.load(spec_path)\n",
    "\n",
    "def hasBird_debug(spec, threshold=16):\n",
    "    img = spec.copy()\n",
    "\n",
    "    # STEP 1: 原始频谱图\n",
    "    visualize_step(\"Step 1: raw spec\", img)\n",
    "    \n",
    "    # STEP 2: Median blur\n",
    "    img = cv2.medianBlur(img, 5)\n",
    "    visualize_step(\"Step 2: after Median Blur\", img)\n",
    "\n",
    "    # STEP 3: Median threshold\n",
    "    col_median = np.median(img, axis=0, keepdims=True)\n",
    "    row_median = np.median(img, axis=1, keepdims=True)\n",
    "    img[img < row_median * 1.2] = 0\n",
    "    img[img < col_median * 1.2] = 0   # baseline = 1.2\n",
    "    img[img > 0] = 1\n",
    "    visualize_step(\"Step 3: after Median threshold\", img)\n",
    "\n",
    "    # STEP 4: Remove isolated pixels\n",
    "    struct = np.ones((3, 3))\n",
    "    id_regions, num_ids = ndimage.label(img, structure=struct)\n",
    "    id_sizes = np.array(ndimage.sum(img, id_regions, range(num_ids + 1)))\n",
    "    area_mask = (id_sizes == 1)\n",
    "    img[area_mask[id_regions]] = 0\n",
    "    visualize_step(\"Step 4: after Remove isolated pixels\", img)\n",
    "\n",
    "    # STEP 5: Morphological closing\n",
    "    img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, np.ones((5, 5), np.float32))\n",
    "    visualize_step(\"Step 5: after Morphological closing\", img)\n",
    "\n",
    "    # STEP 6: Frequency crop (keeping middle frequency range)\n",
    "    img = img[8:-85, :]\n",
    "    visualize_step(\"Step 6: after Frequency crop \", img)\n",
    "\n",
    "    # STEP 7: Count active rows\n",
    "    row_max = np.max(img, axis=1)\n",
    "    row_max = ndimage.binary_dilation(row_max, iterations=2).astype(row_max.dtype)\n",
    "    rthresh = row_max.sum()\n",
    "\n",
    "    print(f\"Step 7: Count active rows: {rthresh}\")\n",
    "\n",
    "    # STEP 8: Apply threshold\n",
    "    result = rthresh >= threshold\n",
    "    print(f\"Step 8: has bird? {result}\")\n",
    "    return result\n",
    "\n",
    "# 运行调试版 hasBird 方法\n",
    "hasBird_debug(spec_data, threshold=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------\n",
    "# 在此之前的代码一般只用执行一次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bird Target filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 📌 文件路径\n",
    "input_csv_path = \"E:/AMR/DA/Projekt/data/all_data_meta_clean.csv\"  # 经过 NaN/Inf 过滤和标签修正的文件\n",
    "# input_csv_path = \"E:/AMR/DA/Projekt/data/all_data_meta.csv\"  # 经过 NaN/Inf 过滤和标签修正的文件\n",
    "output_csv_path = \"E:/AMR/DA/Projekt/data/all_data_meta_allTypes.csv\"  # 目标鸟类数据\n",
    "\n",
    "# 🎯 目标鸟类（仅保留这些类别）\n",
    "# selected_birds = [\n",
    "#     \"Eurasian Blue tit\",\n",
    "#     \"Eurasian Bullfinch\",\n",
    "#     \"Great Tit\",\n",
    "#     \"Hawfinch\",\n",
    "#     \"Hooded Crow\",\n",
    "#     \"Stock Dove\",\n",
    "#     \"Background Noise\",\n",
    "# ]\n",
    "\n",
    "selected_birds = [\n",
    "    \"Black-headed Gull\",\n",
    "    \"Canada Goose\",\n",
    "    \"Carrion Crow\",\n",
    "    \"Common Blackbird\",\n",
    "    \"Common Chaffinch\",\n",
    "    \"Common Kingfisher\",\n",
    "    \"Common Redstart\",\n",
    "    \"Common Wood Pigeon\",\n",
    "    \"Dunnock\",\n",
    "    \"Eurasian Blackcap\",\n",
    "    \"Eurasian Blue tit\",\n",
    "    \"Eurasian Bullfinch\",\n",
    "    \"Eurasian Coot\",\n",
    "    \"Eurasian Golden Oriole\",\n",
    "    \"Eurasian Jay\",\n",
    "    \"Eurasian Nuthatch\",\n",
    "    \"Eurasian Siskin\",\n",
    "    \"Eurasian Treecreeper\",\n",
    "    \"Eurasian Wren\",\n",
    "    \"European Goldfinch\",\n",
    "    \"European Robin\",\n",
    "    \"Goldcrest\",\n",
    "    \"Great Spotted Woodpecker\",\n",
    "    \"Great Tit\",\n",
    "    \"Hawfinch\",\n",
    "    \"Hooded Crow\",\n",
    "    \"Long-tailed Tit\",\n",
    "    \"Mallard\",\n",
    "    \"Marsh Tit\",\n",
    "    \"Redwing\",\n",
    "    \"Rook\",\n",
    "    \"Short-toed Treecreeper\",\n",
    "    \"Stock Dove\",\n",
    "    \"Background Noise\",\n",
    "    \"Background Noise\",\n",
    "]\n",
    "\n",
    "# 🎯 vocalization 过滤条件（根据你的实际需求修改）\n",
    "selected_vocalization = [\"Call\", \"Song\", \"Alarm call\", \"Flight call\", \"Begging call\", \"none\"]  # 假设你只想保留 vocalization 为 'call' 或 'song' 的样本\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(input_csv_path)\n",
    "print(f\"📊 过滤前数据: {len(df)} 条\")\n",
    "\n",
    "# 只保留目标鸟类\n",
    "df_filtered = df[df[\"bird_name\"].isin(selected_birds)]\n",
    "\n",
    "# 只保留符合 vocalization 条件的数据\n",
    "df_filtered = df_filtered[df_filtered[\"vocalization\"].isin(selected_vocalization)]\n",
    "\n",
    "# 保存筛选后的数据\n",
    "df_filtered.to_csv(output_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ 目标鸟类和 vocalization 筛选完成，生成 {output_csv_path}（保留 {len(df_filtered)} 条数据）\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 📌 你的 CSV 文件路径\n",
    "data_csv_path = \"E:/AMR/DA/Projekt/data/train_list_for_zoom006_0324.csv\"\n",
    "\n",
    "# ✅ **检查文件是否存在**\n",
    "if not os.path.exists(data_csv_path):\n",
    "    print(f\"❌ 错误: 找不到文件 {data_csv_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# 读取 CSV 数据\n",
    "df = pd.read_csv(data_csv_path)\n",
    "\n",
    "# 统计每个类别（bird_name）的样本数量\n",
    "class_counts = df[\"bird_name\"].value_counts().reset_index()\n",
    "class_counts.columns = [\"类别\", \"样本数量\"]\n",
    "\n",
    "# 📌 **打印结果**\n",
    "print(\"📊 各类别样本数量统计：\")\n",
    "print(class_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 📌 文件路径\n",
    "data_csv_path = \"E:/AMR/DA/Projekt/data/all_data_meta_allTypes.csv\"\n",
    "train_csv_path = \"E:/AMR/DA/Projekt/data/train_list_for_zoom006_0324.csv\"\n",
    "test_csv_path = \"E:/AMR/DA/Projekt/data/valid_list_for_zoom006_0324.csv\"\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(data_csv_path)\n",
    "print(f\"📊 原始数据: {len(df)} 条\")\n",
    "\n",
    "# ✅ **定义音频编号解析函数**\n",
    "def get_audio_id(number, bird_name):\n",
    "    if bird_name == \"Background Noise\":\n",
    "        return number\n",
    "    return number.split(\"_\")[0] if \"_\" in number else number\n",
    "\n",
    "# 计算 `audio_id`\n",
    "df[\"audio_id\"] = df.apply(lambda row: get_audio_id(row[\"number\"], row[\"bird_name\"]), axis=1)\n",
    "\n",
    "# 🚀 拆分为普通样本和负样本\n",
    "neg_samples = df[df[\"bird_name\"] == \"Background Noise\"].copy()\n",
    "pos_samples = df[df[\"bird_name\"] != \"Background Noise\"].copy()\n",
    "\n",
    "# ✅ 按 audio_id 进行训练/测试划分（20% 测试集）\n",
    "unique_audio_ids = pos_samples[\"audio_id\"].unique()\n",
    "train_audio_ids, test_audio_ids = train_test_split(unique_audio_ids, test_size=0.2, random_state=2024, shuffle=True)\n",
    "\n",
    "train_df = pos_samples[pos_samples[\"audio_id\"].isin(train_audio_ids)]\n",
    "test_df = pos_samples[pos_samples[\"audio_id\"].isin(test_audio_ids)]\n",
    "\n",
    "# ✅ 限制测试集每类不超过 500 个样本，剩下的回流到训练集（排除碎片泄露）\n",
    "test_limited_df = []\n",
    "train_remainder_df = []\n",
    "test_audio_ids_set = set(test_df[\"audio_id\"])\n",
    "\n",
    "for bird_name in test_df[\"bird_name\"].unique():\n",
    "    class_samples = test_df[test_df[\"bird_name\"] == bird_name]\n",
    "\n",
    "    if len(class_samples) > 500:\n",
    "        test_limited = class_samples.sample(n=500, random_state=2024)\n",
    "        remainder = class_samples.drop(test_limited.index)\n",
    "\n",
    "        # ❗ 移除与 test_limited 相同 audio_id 的碎片，避免回流污染\n",
    "        remainder = remainder[~remainder[\"audio_id\"].isin(test_limited[\"audio_id\"])]\n",
    "        train_remainder = remainder\n",
    "    else:\n",
    "        test_limited = class_samples\n",
    "        train_remainder = pd.DataFrame()\n",
    "\n",
    "    test_limited_df.append(test_limited)\n",
    "    train_remainder_df.append(train_remainder)\n",
    "\n",
    "# 合并测试集 & 安全回流训练集\n",
    "test_df = pd.concat(test_limited_df, ignore_index=True)\n",
    "train_df = pd.concat([train_df] + train_remainder_df, ignore_index=True)\n",
    "\n",
    "# ✅ 背景噪声划分\n",
    "neg_audio_ids = neg_samples[\"audio_id\"].unique()\n",
    "print(\"\\n🔍 负样本 `audio_id` 统计：\")\n",
    "print(pd.Series(neg_audio_ids).value_counts())\n",
    "\n",
    "if len(neg_audio_ids) >= 2:\n",
    "    train_neg_ids, test_neg_ids = train_test_split(neg_audio_ids, test_size=0.2, random_state=2024, shuffle=True)\n",
    "    train_neg_df = neg_samples[neg_samples[\"audio_id\"].isin(train_neg_ids)]\n",
    "    test_neg_df = neg_samples[neg_samples[\"audio_id\"].isin(test_neg_ids)]\n",
    "else:\n",
    "    print(f\"⚠️ 负样本数量不足 ({len(neg_audio_ids)} 个)，全部放入训练集！\")\n",
    "    train_neg_df = neg_samples\n",
    "    test_neg_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# ✅ 合并训练 & 测试集\n",
    "train_df = pd.concat([train_df, train_neg_df], ignore_index=True)\n",
    "test_df = pd.concat([test_df, test_neg_df], ignore_index=True)\n",
    "\n",
    "# ✅ 删除辅助列 audio_id\n",
    "train_df.drop(columns=[\"audio_id\"], inplace=True)\n",
    "test_df.drop(columns=[\"audio_id\"], inplace=True)\n",
    "\n",
    "# ✅ 保存 CSV 文件\n",
    "os.makedirs(os.path.dirname(train_csv_path), exist_ok=True)\n",
    "train_df.to_csv(train_csv_path, index=False, encoding=\"utf-8\")\n",
    "test_df.to_csv(test_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ 训练集已保存至: {train_csv_path}, 样本数: {len(train_df)}\")\n",
    "print(f\"✅ 测试集已保存至: {test_csv_path}, 样本数: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 📌 配置\n",
    "data_csv_path = \"E:/AMR/DA/Projekt/data/all_data_meta_filtered.csv\"  # 目标鸟类数据\n",
    "output_dir = \"E:/AMR/DA/Projekt/data\"  # 交叉验证数据保存目录\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(data_csv_path)\n",
    "print(f\"📊 原始数据: {len(df)} 条\")\n",
    "\n",
    "# 提取音频编号（去掉 segX）\n",
    "df[\"audio_id\"] = df[\"number\"].apply(lambda x: x.split(\"_\")[0] if \"_\" in x else x)\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 🚀 **按音频编号进行划分（确保所有片段在同一个集合）**\n",
    "unique_audio_ids = df[\"audio_id\"].unique()\n",
    "part1_audio_ids, part2_audio_ids = train_test_split(unique_audio_ids, test_size=0.5, random_state=2024, shuffle=True)\n",
    "\n",
    "# **分配数据**\n",
    "part1_df = df[df[\"audio_id\"].isin(part1_audio_ids)].drop(columns=[\"audio_id\"])\n",
    "part2_df = df[df[\"audio_id\"].isin(part2_audio_ids)].drop(columns=[\"audio_id\"])\n",
    "\n",
    "# **保存文件**\n",
    "part1_csv_path = os.path.join(output_dir, \"crossval_part1.csv\")\n",
    "part2_csv_path = os.path.join(output_dir, \"crossval_part2.csv\")\n",
    "\n",
    "part1_df.to_csv(part1_csv_path, index=False, encoding=\"utf-8\")\n",
    "part2_df.to_csv(part2_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ 交叉验证数据集 1: {part1_csv_path}, 样本数: {len(part1_df)}\")\n",
    "print(f\"✅ 交叉验证数据集 2: {part2_csv_path}, 样本数: {len(part2_df)}\")\n",
    "print(\"\\n🎯 交叉验证数据集划分完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and valid distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"E:/AMR/DA/Projekt/data/train_list_for_zoom006_0315.csv\")\n",
    "valid_df = pd.read_csv(\"E:/AMR/DA/Projekt/data/valid_list_for_zoom006_0315.csv\")\n",
    "\n",
    "print(\"训练集类别分布:\")\n",
    "print(train_df[\"bird_name\"].value_counts())\n",
    "\n",
    "print(\"\\n测试集类别分布:\")\n",
    "print(valid_df[\"bird_name\"].value_counts())\n",
    "\n",
    "train_df_refine = pd.read_csv(\"E:/AMR/DA/Projekt/data/train_list_for_zoom006_100_nofreefiled_refine_th12.csv\")\n",
    "valid_df_refine = pd.read_csv(\"E:/AMR/DA/Projekt/data/valid_list_for_zoom006_100_nofreefiled_refine_th12.csv\")\n",
    "\n",
    "print(\"训练集类别分布refine:\")\n",
    "print(train_df_refine[\"bird_name\"].value_counts())\n",
    "print(\"\\n测试集类别分布refine:\")\n",
    "print(valid_df_refine[\"bird_name\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "\n",
    "# 📌 你的音频数据存放的根目录（修改为你的路径）\n",
    "audio_root = \"E:/AMR/DA/Projekt/data/Audio_files\"\n",
    "\n",
    "# 📌 统计音频编号出现的文件夹\n",
    "xc_file_map = collections.defaultdict(set)\n",
    "\n",
    "# 🚀 遍历所有 vocalization 文件夹\n",
    "for vocalization in os.listdir(audio_root):\n",
    "    vocalization_path = os.path.join(audio_root, vocalization)\n",
    "    \n",
    "    if os.path.isdir(vocalization_path):  # 确保是文件夹\n",
    "        for file in os.listdir(vocalization_path):\n",
    "            if file.endswith(\".wav\") and file.startswith(\"XC\"):\n",
    "                xc_id = file.split(\".\")[0]  # 获取音频编号，例如 XC123456\n",
    "                xc_file_map[xc_id].add(vocalization)  # 记录该音频在哪些 vocalization 中出现\n",
    "\n",
    "# 🚀 找到重复出现的音频\n",
    "duplicates = {xc: v for xc, v in xc_file_map.items() if len(v) > 1}\n",
    "\n",
    "# 📌 按照出现的 vocalization 数量排序\n",
    "sorted_duplicates = sorted(duplicates.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "# ✅ 打印结果\n",
    "print(f\"🔍 共有 {len(sorted_duplicates)} 个音频文件出现在多个 vocalization 目录中:\\n\")\n",
    "for xc_id, voc_types in sorted_duplicates:\n",
    "    print(f\"{xc_id}: 出现在 {len(voc_types)} 个 vocalization 中 → {', '.join(voc_types)}\")\n",
    "\n",
    "# 📌 保存结果到 CSV\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([(xc_id, len(voc_types), \", \".join(voc_types)) for xc_id, voc_types in sorted_duplicates],\n",
    "                  columns=[\"XC_ID\", \"Vocalization Count\", \"Vocalization Types\"])\n",
    "df.to_csv(\"E:/AMR/DA/Projekt/data/duplicate_audio_files.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n✅ 统计完成，结果已保存至 `duplicate_audio_files.csv`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 📌 你的 CSV 文件路径\n",
    "csv_path = \"E:/AMR/DA/Projekt/data/train_meta_100.csv\"\n",
    "output_csv_path = \"E:/AMR/DA/Projekt/data/train_meta_100_deduplicated.csv\"\n",
    "\n",
    "# ✅ 读取 CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# ✅ 提取 `XC` 编号\n",
    "df[\"XC_ID\"] = df[\"number\"].apply(lambda x: x.split(\"_\")[0] if \"_\" in x else x)\n",
    "\n",
    "# ✅ **去重（保留第一次出现的 XC_ID）**\n",
    "df_deduplicated = df.drop_duplicates(subset=\"XC_ID\", keep=\"first\")\n",
    "\n",
    "# ✅ **删除 `XC_ID` 辅助列**\n",
    "df_deduplicated = df_deduplicated.drop(columns=[\"XC_ID\"])\n",
    "\n",
    "# ✅ 保存去重后的 CSV\n",
    "df_deduplicated.to_csv(output_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ 去重完成！原始数据: {len(df)} 条 → 处理后: {len(df_deduplicated)} 条\")\n",
    "print(f\"📄 结果已保存至: {output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
